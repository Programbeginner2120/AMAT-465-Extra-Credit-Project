---
title: "project-final"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading dataset from directory
```{r}
setwd("/Users/matthewkilleen/Desktop/School/UAlbany Folder/Fall-2021-Semester/AMAT-465-Applied-Statistics")
project_dataset <- read.table("./Datasets/project-dataset.csv", sep=",")
```


# Editting variable names
```{r}
colnames(project_dataset) = c('Fat_Percent', 'Density', 'Age', 'Weight',
                            'Height', 'Neck Circ.', 'Chest Circ.',
                            'Abdomen 2 Circ.', 'Hip Circ.', 'Thigh Circ.',
                            'Knee Circ.', 'Ankle Circ.', 'Bicep Circ.',
                            'Forearm Circ.', 'Wrist Circ.')
```


We know that Fat_Percent is determined via the rearranged formula
for density, i.e. 100*B = 495/D - 450 where B is Fat_Percent and D is Density.
That being said, Density has nearly 100% correlation with Fat_Percent. 
Furthermore, our goal is to predict Fat_Percent as calculated using the formula
containing Density using the other body measurements. Therefore, we must delete
Density from our data set.

In addition, when looking at the data set we can see that the data points in rows
39 and 42 are outliers. The data point in row 39 has an extremely high weight
value of 363.15 lbs and other inflated measurements (to a lesser extent), 
while the data point at row 42 has an extremely low height value of 29.5 inches.
Therefore, I feel that it would be best to omit these data points from the 
data set.


```{r}
# Getting rid of column 'Density'
project_dataset = project_dataset[, -grep('Density', colnames(project_dataset))]

# Getting rid of data point with extremely high weight value, i.e. row 39
project_dataset = project_dataset[-c(which(project_dataset$Weight == max(project_dataset$Weight)),which(project_dataset$Height == min(project_dataset$Height))), ]
```


Finally, for all data points, I decided to remove all data points that had values
for explanatory variables more than 5 standard deviations away from the mean. This
is to ensure that the data for the explanatory variables is within the normal distribution.


```{r}
for (val in 2:length(project_dataset)){
  mean = mean(project_dataset[, val])
  sdv = sd(project_dataset[, val])
  list = c(which(project_dataset[, val] > mean + 5 * sdv))
  if (length(list) > 0)
    project_dataset = project_dataset[-list, ]
}
```


In order to derive optimal model(s), I will later carry out cross validation. Therefore,
I will split the data set into training and testing data sets; the training data set
will have 80% of the data and the testing data set will have the remaining 20%.

```{r}
set.seed(100)
n<-length(project_dataset$`Fat_Percent`)
cvindex<-sample(1:n,.8*n,replace=FALSE)

train<-project_dataset[cvindex,]
test<-project_dataset[-cvindex,]
```


Before we create an initial model for the dataset, let's take a look at the output
of pairs.R to get a sense of the relationships between variables:


# Partioning data to call pairs.r in a way to make plots more visible
```{r}
#TODO: Make this look better later, it works for now
start = 2
end = if (length(project_dataset) < 5) length(project_dataset) else 5
source('pairs.R')
while (TRUE){
  pairs(project_dataset[, c(1, start:end)],panel=panel.smooth,diag.panel=panel.hist,lower.panel=panel.cor)
  start = end + 1
  if (start == length(project_dataset))
    end = start
  else if (length(project_dataset) - start < 3)
    break
  else
    end = start + 3
}
```


RELATIONSHIPS: We can see that a decent amount of the explanatory variables have a linear relationship with the response variable. However, there are some variables which have a curved relationship with the response variable (there are also a few explanatory variables that have
curved relationships with other explanatory variables, though to a lesser extent).
That being said, I believe it will be worthwhile to investigate the use of polynomial
terms in the model as well as the transformation of the response and/or explanatory variables.

CORRELATIONS: It seems that the variables in the data set are relatively highly correlated
with one another, which may lead to negative coefficients for the explanatory variables
and / or intercept. This may also lead to a model that yields a significant F statistic
for the global significance test for the model but has many insignificant explanatory variables.


Now that we've taken a look at the interactions between our variables and made
some remarks about them, let's create a naive model where we use all variables:


# generating and plotting model wherein Fat_Percent is the response variable
# and all other variables are predictor variables
```{r}
naive_model = lm(`Fat_Percent` ~ ., data = train)
summary(naive_model)
```


Looking at the above model, we can see a few interesting things:

1. A few of the coefficients for the explanatory variables are negative in our
naive model (namely Age, Abdomen 2 Circ., Thigh Circ., Knee Circ., Ankle Circ., Bicep Circ.
and Forearm Circ.)
2. Many of the explanatory variables fail to be significant in the model
3. The glabal F test for the model indicates that the model is significant with
all of these variables

This indicates that multicolinearity (correlation) is present within the model.

Looking at the other metrics produced for the model, we can see that the sum of
square errors (AKA residual standard error) is low, indicating that the model
fits the data well. In addition, the multiple R-squared and the adjusted
R-squared have relatively high magnitudes; this indicates that there is a 
relatively high proportion of the variance for Fat_Percent that is explained
by the model's explanatory variables.


Let's take a look at some of the diagnostic plots we can produce for the model:


```{r}
par(mfrow = c(2,2))
plot(naive_model)
```


Residuals vs. Fitted plot: Used to determine if the residuals exhibit a non-linear
pattern; the red line across the center of the plot is pretty horizontal, therefore
it would be reasonable to assume that the residuals follow a linear pattern

Normal Q-Q plot: Used to determine if the residuals of the regression model are
normally distributed; if the points fall roughly on the diagonal line, then
we can assume they are normally distributed. Some of the points at both ends of 
the line start to deviate a bit from the diagonal, though most lie nearly on it
so it's safe to assume they are normally distributed.

Scale-Location plot: Used to check the assumption of homoscedasticity / equal
variance among the residuals of the regression model. If the red line is roughly
horizontal across the plot, then the assumption is likely met. The red line in
our plot is nearly horizontal, therefore it is safe to assume constant variance.

Residuals vs. Leverage plot: Used to identify influential observations. If any
points in this plot fall outside of Cook's distance (the dashed line(s)), then
the point(s) are influential. In our plot, none of the points cross and therefore
there aren't any overly influential points in the data set.


To test our hypotheses that the residuals are normally distributed and have
constant variance, we can use the Shapiro-Wilks test and Breusch-Pagan tests
respectively. In the Shapiro-Wilks test, the null hypothesis is that the
residuals are normally distributed, the alternative hypothesis being they are
not normally distributed. In the Breusch-Pagan test, the null hypothesis is that
the residuals have constant variance, the alternative being they have non 
constant variance. If the p-values < 0.5 for either of these tests, we reject
the null hypothesis and accept the alternative. Otherwise, we fail to reject
the null hypothesis.


Let's take a look at the naive model's residuals plot:

```{r}
plot(naive_model$residuals, main="Naive Model Plot of Residuals")
```

We can see that there is no discernable pattern within the plot of residuals, indicating
no correlation


# Conducting Shapiro-Wilks and Breusch-Pagan tests on residuals
```{r}
library(lmtest)
shapiro.test(naive_model$residuals)
bptest(naive_model)
```

We can see that we fail to reject the null hypothesis for each test, therefore
we conclude that the residuals of our model are normally distributed and have
constant variance.


TAKING A LOOK AT HOW THE NAIVE MODEL FITS THE TRAINING DATA:

```{r}
library(car)
scatterplot(predict(naive_model), train$Fat_Percent, smooth = FALSE, main = "Naive Model Fit (Train)")
pred_interval <- predict(naive_model, newdata=train, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```



REMEDYING CURVED RELATIONSHIPS:

Before we start doing cross validation, let's see if we can add any
interactions, polynomial terms or transform our variables in order to produce
a more easily interpretable model.

Interaction Effects:

Since there are no categorical variables in the data set we are using (all
variables are numeric), there is no need to include interaction effects in our
model.


When we used pairs.R before, we plotted the relationships between our variables.
Let's take a closer look at the relationships between the explanatory variables
and the response variable:

# Plotting explanatory variables vs. response variable
```{r}
attach(train)
par(mfrow=c(2,2))
for (val in 2:length(train)){
  plot(train[, val], `Fat_Percent`, xlab = colnames(train[val]))
  lines(lowess(train[, val], `Fat_Percent`), col="blue")
}
```

We can see a somewhat curved relationship in some of the plots, therefore let's
look into whether we should try and transform our variables.


Transformation of Response:

# Calling boxCox() on response variable
```{r}
library(car)
boxCox(naive_model)
```

The boxCox plot indicates that an optimal choice for lambda is extremtly close to 1, indicating that raising our response variable to the power of 1 would be optimal. The reason that we'd transform our response variable would be in an effort to achieve
constant variance and normality of the residuals for our model. We already have
these, however, and we don't want to risk losing this. Therefore, it would be
best to leave the response variable as it is.


Transformations on Explanatory Variables and Polynomial Terms:

As we saw in the plots of our explanatory variables vs. the response variable,
some of the explanatory variables have a somewhat curvilinear relationship with
the response variable, while the rest are more or less linear. This would indicate
that we should attempt to transform some of our explanatory variables and / or
add some polynomial terms into our model. The two that I have identified as most
ploblematic are Neck Circ. and Bicep Circ. Both of these curves seem
to have the shape of a cubic function stretched out across the x axis.
Therefore, I am going to add a cubic polynomial term for both.

# Plot of the problematic explanatory variables
```{r}
attach(train)
par(mfrow=c(2,1))
plot(.01 * (train[, 5] + train[, 5]^2 + train[, 5]^3), `Fat_Percent`, xlab = colnames(train[5]))
lines(lowess(.01 * (train[, 5] + train[, 5]^2 + train[, 5]^3), `Fat_Percent`), col="blue")

plot(.01 * (train[, 13] + train[, 13]^2 + train[, 13]^3), `Fat_Percent`, xlab = colnames(train[5]))
lines(lowess(.01 * (train[, 13] + train[, 13]^2 + train[, 13]^3), `Fat_Percent`), col="blue")
```

# Creating model with polynomial terms
```{r}
poly_model = lm(Fat_Percent ~ . -`Neck Circ.` + poly(.01 * `Neck Circ.`, 3)
                -`Forearm Circ.` + poly(.01 * `Forearm Circ.`, 3), data = train)
summary(poly_model)
```

# Plotting diagnostics for poly model
```{r}
par(mfrow=c(2,2))
plot(poly_model)
```

# Conducting Breusch-Pagan and Shapiro-Wilks tests to check for constant variance
# and normally distributed residuals respectively
```{r}
library(lmtest)
shapiro.test(poly_model$residuals)
bptest(poly_model)
```


TAKING A LOOK AT MODEL RESIDUALS:

```{r}
plot(poly_model$residuals, ylab = "Model Residuals", main = "Polynomial Model Plot of residuals")
lines(lowess(poly_model$residuals))
```


TAKING A LOOK AT HOW THE MODEL FITS THE TRAINING DATA:

```{r}
library(car)
scatterplot(predict(poly_model), train$Fat_Percent, smooth = FALSE, main = "Polynomial Model Fit (Train)")
pred_interval <- predict(poly_model, newdata=train, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

There doesn't seem to be any difference or improvement, so let's just stick with
the naive model.

Cross Validation:

```{r}
fullMSE<-summary(naive_model)$sig^2      # needed to compute Cp


backsel = step(naive_model, direction = "backward")
bothsel<-step(naive_model, direction = "both")


source("modelselectionfunctions.R")
library(leaps)
lp2<-regsubsets(Fat_Percent ~ ., nbest=3, data=train, really.big=T)


lp2matrix<-matrix.selection(lp2,Xnames=lp2$xnames[-1],Yname='Fat_Percent',fullMSE,train)


size<-apply((summary(lp2)$which*1),1,sum)                             #size=p+1

ibestbic<-which(summary(lp2)$bic==min(summary(lp2)$bic))
ibestadjr2<-which(summary(lp2)$adjr2==max(summary(lp2)$adjr2))
ibestcp<- which(abs(lp2matrix$Cp - size) == min(abs(lp2matrix$Cp - size)))
ibestaic<-which(lp2matrix$AIC==min(lp2matrix$AIC))

foo <- summary(lp2)$which[ibestbic, ]
form <- lp2$xnames[foo][-1]          #remove the intercept
form <- paste(form, collapse = " + ")
form <- paste("Fat_Percent~", form)
bicmod<- lm(as.formula(form), data=train)


foo <- summary(lp2)$which[ibestaic, ]
form <- lp2$xnames[foo][-1] 
form <- paste(form, collapse = " + ")
form <- form <- paste("Fat_Percent~", form)
aicmod<- lm(as.formula(form),data=train)


foo <- summary(lp2)$which[ibestadjr2, ]
form <- lp2$xnames[foo][-1] 
form <- paste(form, collapse = " + ")
form <- form <- paste("Fat_Percent~", form)
adjr2mod<- lm(as.formula(form),data=train)


foo <- summary(lp2)$which[ibestcp, ]
form <- lp2$xnames[foo][-1] 
form <- paste(form, collapse = " + ")
form <- form <- paste("Fat_Percent~", form)
cpmod<- lm(as.formula(form),data=train)



rbind(bsel=Criteria(backsel,fullMSE,label=T), bicm=Criteria(bicmod,fullMSE,label=T), 
      aicm=Criteria(aicmod,fullMSE),
      adjr2m=Criteria(adjr2mod,fullMSE),
      cpm=Criteria(cpmod,fullMSE))

#MSEpred = mean ((bicmod$fitted.values - test)^2) 

bic_values = bicmod$fitted.values[test$`Fat_Percent`]
aic_values = aicmod$fitted.values[test$`Fat_Percent`]
adj_values = adjr2mod$fitted.values[test$`Fat_Percent`]
cp_values = cpmod$fitted.values[test$`Fat_Percent`]

MSEpredbic = mean((bic_values - test$`Fat_Percent`)^2)
MSEpredaic = mean((aic_values - test$`Fat_Percent`)^2)
MSEpredadj = mean((adj_values - test$`Fat_Percent`)^2)
MSEpredcp = mean((cp_values - test$`Fat_Percent`)^2)
```


```{r}
summary(backsel)
# summary(bothsel) # backsel and bothsel produce same models, therefore just use backsel
summary(bicmod)
summary(aicmod)
summary(adjr2mod)
summary(cpmod)
```

These models were selected based upon the best adjusted R-squared, Mallow's CP,
Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) respectively.
As we did with the naive model, let's take a look at these models diagnostic plots:

```{r}
par(mfrow=c(2,2))
plot(backsel)
# plot(bothsel)
plot(bicmod)
plot(aicmod)
plot(adjr2mod)
plot(cpmod)
```

We can see that all the plots indicate constant variance and no overly significant outliers,
though indicate there may be non-normally distributed residuals.

```{r}
library(lmtest)
shapiro.test(backsel$residuals)
bptest(backsel)
# shapiro.test(bothsel$residuals)
# bptest(bothsel)
shapiro.test(bicmod$residuals)
bptest(bicmod)
shapiro.test(aicmod$residuals)
bptest(aicmod)
shapiro.test(adjr2mod$residuals)
bptest(adjr2mod)
shapiro.test(cpmod$residuals)
bptest(cpmod)
```

In addition, we can see that the Breusch-Pagan and Shapiro-Wilks tests (tests for
constant variance and normality of residuals respectively) fail to reject the null hypothesis
and reject the null hypothesis respectively at the 5% significance level. This indicates that
the residuals of the model have constant variance, though are not normally distributed (face some level of correlation)

```{r}
par(mfrow=c(2,3))
plot(backsel$residuals, main="Back selection residuals")
plot(bicmod$residuals, main="BIC model residuals")
plot(aicmod$residuals, main="AIC model residuals")
plot(adjr2mod$residuals, main="Adjusted R-Squared model residuals")
plot(cpmod$residuals, main="Cp model residuals")
```


Let's see how these models perfrom on the test data:

```{r}
scatterplot(predict(backsel, test), test$Fat_Percent, smooth = FALSE, main="Back Selection Testing Fit")
pred_interval <- predict(backsel, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

```{r}
scatterplot(predict(cpmod, test), test$Fat_Percent, smooth = FALSE, main="Cp Model Testing Fit")
pred_interval <- predict(cpmod, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

```{r}
scatterplot(predict(aicmod, test), test$Fat_Percent, smooth = FALSE, main="AIC Model Testing Fit")
pred_interval <- predict(aicmod, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

```{r}
scatterplot(predict(bicmod, test), test$Fat_Percent, smooth = FALSE, main="BIC Model Testing Fit")
pred_interval <- predict(bicmod, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

```{r}
scatterplot(predict(adjr2mod, test), test$Fat_Percent, smooth = FALSE, main="Adjusted R-Squared Model Testing Fit")
pred_interval <- predict(adjr2mod, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

```{r}
scatterplot(predict(naive_model, test), test$Fat_Percent, smooth = FALSE, main = "Naive Model Fit (Test)")
pred_interval <- predict(naive_model, newdata=test, interval="prediction", level = 0.95)
lines(pred_interval[,1], pred_interval[,2], col="red")
lines(pred_interval[,1], pred_interval[,3], col="red")
```

We can see that all models predict the data quite well, with all points being within the generated 95% prediction intervals (indicated by red bounds)